{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 21:20:59 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "/home/jovyan/fairseq/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n",
      "2024-08-15 21:21:00 | INFO | fairseq.file_utils | loading archive file /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/\n",
      "/home/jovyan/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "2024-08-15 21:21:19 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2024-08-15 21:21:21 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/myleott/tensorboard_logs/2020-05-23/adaptive_lm_wiki103.fp16.no_c10d.transformer_lm_wiki103.adaptive_loss.nag.clip0.1.warmup16000.initlr1e-07.sampletok3072.breaknone.maxtok3072.updatefreq3.seed2.ngpu8', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16480', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3072, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 0, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3072, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 286000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [3], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/myleott/2020-05-23/adaptive_lm_wiki103.fp16.no_c10d.transformer_lm_wiki103.adaptive_loss.nag.clip0.1.warmup16000.initlr1e-07.sampletok3072.breaknone.maxtok3072.updatefreq3.seed2.ngpu8', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_wiki103', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 16, 'decoder_attention_heads': 8, 'decoder_normalize_before': True, 'no_decoder_final_norm': True, 'adaptive_softmax_cutoff': '20000,60000', 'adaptive_softmax_dropout': 0.2, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': False, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': True, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': '20000,60000', 'tie_adaptive_weights': True, 'tie_adaptive_proj': True, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 3072, 'max_target_positions': 3072, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': '/base-vol-2/fairseq/models/adaptive_lm_wiki103.v2', 'sample_break_mode': 'none', 'tokens_per_sample': 3072, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 3072, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 2, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 0, 'tpu': False, 'use_plasma_view': True, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'adaptive_loss', 'sentence_avg': False, 'ddp_backend': 'no_c10d'}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 16000, 'warmup_init_lr': 1e-07, 'lr': [0.0001], 'min_lr': 0.0, 't_mult': 2.0, 'lr_period_updates': 270000.0, 'lr_shrink': 0.75, 'max_update': 286000}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.transformer_lm import TransformerLanguageModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n",
    "local_rank = 0\n",
    "device = \"cuda:\" + str(local_rank) if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(device)\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "#Load Pretrained Model\n",
    "fairseq_model = TransformerLanguageModel.from_pretrained('/base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorHubInterface(\n",
      "  (models): ModuleList(\n",
      "    (0): TransformerLanguageModel(\n",
      "      (decoder): TransformerDecoder(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (embed_tokens): AdaptiveInput(\n",
      "          (embeddings): ModuleList(\n",
      "            (0): Sequential(\n",
      "              (0): Embedding(20000, 1024, padding_idx=1)\n",
      "              (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Embedding(40000, 256)\n",
      "              (1): Linear(in_features=256, out_features=1024, bias=False)\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Embedding(207744, 64)\n",
      "              (1): Linear(in_features=64, out_features=1024, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (embed_positions): SinusoidalPositionalEmbedding()\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x TransformerDecoderLayerBase(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (adaptive_softmax): AdaptiveSoftmax(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (lsm): LogSoftmax(dim=1)\n",
      "          (head): TiedHeadModule(\n",
      "            (word_proj): TiedLinear()\n",
      "            (class_proj): Linear(in_features=1024, out_features=2, bias=False)\n",
      "          )\n",
      "          (tail): ModuleList(\n",
      "            (0-1): 2 x Sequential(\n",
      "              (0): TiedLinear()\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): TiedLinear()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(fairseq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(fairseq_model.models[0].decoder.layers[0].activation_dropout_module.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_namearguments:\n",
      "commonarguments:\n",
      "_name None\n",
      "no_progress_bar False\n",
      "log_interval 25\n",
      "log_format json\n",
      "log_file None\n",
      "aim_repo None\n",
      "aim_run_hash None\n",
      "tensorboard_logdir /checkpoint/myleott/tensorboard_logs/2020-05-23/adaptive_lm_wiki103.fp16.no_c10d.transformer_lm_wiki103.adaptive_loss.nag.clip0.1.warmup16000.initlr1e-07.sampletok3072.breaknone.maxtok3072.updatefreq3.seed2.ngpu8\n",
      "wandb_project None\n",
      "azureml_logging False\n",
      "seed 2\n",
      "cpu False\n",
      "tpu False\n",
      "bf16 False\n",
      "memory_efficient_bf16 False\n",
      "fp16 True\n",
      "memory_efficient_fp16 False\n",
      "fp16_no_flatten_grads False\n",
      "fp16_init_scale 128\n",
      "fp16_scale_window None\n",
      "fp16_scale_tolerance 0.0\n",
      "on_cpu_convert_precision False\n",
      "min_loss_scale 0.0001\n",
      "threshold_loss_scale None\n",
      "amp False\n",
      "amp_batch_retries 2\n",
      "amp_init_scale 128\n",
      "amp_scale_window None\n",
      "user_dir None\n",
      "empty_cache_freq 0\n",
      "all_gather_list_size 16384\n",
      "model_parallel_size 1\n",
      "quantization_config_path None\n",
      "profile False\n",
      "reset_logging False\n",
      "suppress_crashes False\n",
      "use_plasma_view False\n",
      "plasma_path /tmp/plasma\n",
      "-------------------------------\n",
      "common_evalarguments:\n",
      "_name None\n",
      "path None\n",
      "post_process None\n",
      "quiet False\n",
      "model_overrides {}\n",
      "results_path None\n",
      "-------------------------------\n",
      "distributed_trainingarguments:\n",
      "_name None\n",
      "distributed_world_size 8\n",
      "distributed_num_procs 1\n",
      "distributed_rank 0\n",
      "distributed_backend nccl\n",
      "distributed_init_method tcp://localhost:16480\n",
      "distributed_port -1\n",
      "device_id 0\n",
      "distributed_no_spawn False\n",
      "ddp_backend no_c10d\n",
      "ddp_comm_hook none\n",
      "bucket_cap_mb 25\n",
      "fix_batches_to_gpus False\n",
      "find_unused_parameters False\n",
      "gradient_as_bucket_view False\n",
      "fast_stat_sync False\n",
      "heartbeat_timeout -1\n",
      "broadcast_buffers False\n",
      "slowmo_momentum None\n",
      "slowmo_base_algorithm localsgd\n",
      "localsgd_frequency 3\n",
      "nprocs_per_node 8\n",
      "pipeline_model_parallel False\n",
      "pipeline_balance None\n",
      "pipeline_devices None\n",
      "pipeline_chunks 0\n",
      "pipeline_encoder_balance None\n",
      "pipeline_encoder_devices None\n",
      "pipeline_decoder_balance None\n",
      "pipeline_decoder_devices None\n",
      "pipeline_checkpoint never\n",
      "zero_sharding none\n",
      "fp16 True\n",
      "memory_efficient_fp16 False\n",
      "tpu False\n",
      "no_reshard_after_forward False\n",
      "fp32_reduce_scatter False\n",
      "cpu_offload False\n",
      "use_sharded_state False\n",
      "not_fsdp_flatten_parameters False\n",
      "-------------------------------\n",
      "datasetarguments:\n",
      "_name None\n",
      "num_workers 1\n",
      "skip_invalid_size_inputs_valid_test True\n",
      "max_tokens 3072\n",
      "batch_size None\n",
      "required_batch_size_multiple 8\n",
      "required_seq_len_multiple 1\n",
      "dataset_impl None\n",
      "data_buffer_size 0\n",
      "train_subset train\n",
      "valid_subset valid\n",
      "combine_valid_subsets None\n",
      "ignore_unused_valid_subsets False\n",
      "validate_interval 1\n",
      "validate_interval_updates 0\n",
      "validate_after_updates 0\n",
      "fixed_validation_seed None\n",
      "disable_validation False\n",
      "max_tokens_valid 3072\n",
      "batch_size_valid None\n",
      "max_valid_steps None\n",
      "curriculum 0\n",
      "gen_subset test\n",
      "num_shards 1\n",
      "shard_id 0\n",
      "grouped_shuffling False\n",
      "update_epoch_batch_itr True\n",
      "update_ordered_indices_seed False\n",
      "-------------------------------\n",
      "optimizationarguments:\n",
      "_name None\n",
      "max_epoch 0\n",
      "max_update 286000\n",
      "stop_time_hours 0.0\n",
      "clip_norm 0.1\n",
      "sentence_avg False\n",
      "update_freq [3]\n",
      "lr [0.0001]\n",
      "stop_min_lr -1.0\n",
      "use_bmuf False\n",
      "skip_remainder_batch False\n",
      "debug_param_names False\n",
      "-------------------------------\n",
      "checkpointarguments:\n",
      "_name None\n",
      "save_dir /checkpoint/myleott/2020-05-23/adaptive_lm_wiki103.fp16.no_c10d.transformer_lm_wiki103.adaptive_loss.nag.clip0.1.warmup16000.initlr1e-07.sampletok3072.breaknone.maxtok3072.updatefreq3.seed2.ngpu8\n",
      "restore_file checkpoint_last.pt\n",
      "continue_once None\n",
      "finetune_from_model None\n",
      "reset_dataloader False\n",
      "reset_lr_scheduler False\n",
      "reset_meters False\n",
      "reset_optimizer False\n",
      "optimizer_overrides {}\n",
      "save_interval 1\n",
      "save_interval_updates 0\n",
      "keep_interval_updates -1\n",
      "keep_interval_updates_pattern -1\n",
      "keep_last_epochs -1\n",
      "keep_best_checkpoints -1\n",
      "no_save False\n",
      "no_epoch_checkpoints False\n",
      "no_last_checkpoints False\n",
      "no_save_optimizer_state False\n",
      "best_checkpoint_metric loss\n",
      "maximize_best_checkpoint_metric False\n",
      "patience -1\n",
      "checkpoint_suffix \n",
      "checkpoint_shard_count 1\n",
      "load_checkpoint_on_all_dp_ranks False\n",
      "write_checkpoints_asynchronously False\n",
      "model_parallel_size 1\n",
      "-------------------------------\n",
      "bmufarguments:\n",
      "_name None\n",
      "block_lr 1.0\n",
      "block_momentum 0.875\n",
      "global_sync_iter 50\n",
      "warmup_iterations 500\n",
      "use_nbm False\n",
      "average_sync False\n",
      "distributed_world_size 8\n",
      "-------------------------------\n",
      "generationarguments:\n",
      "_name None\n",
      "beam 5\n",
      "beam_mt 0\n",
      "nbest 1\n",
      "max_len_a 0.0\n",
      "max_len_b 200\n",
      "max_len_a_mt 0.0\n",
      "max_len_b_mt 200\n",
      "min_len 1\n",
      "match_source_len False\n",
      "unnormalized False\n",
      "no_early_stop False\n",
      "no_beamable_mm False\n",
      "lenpen 1.0\n",
      "lenpen_mt 1.0\n",
      "unkpen 0.0\n",
      "replace_unk None\n",
      "sacrebleu False\n",
      "score_reference False\n",
      "prefix_size 0\n",
      "no_repeat_ngram_size 0\n",
      "sampling False\n",
      "sampling_topk -1\n",
      "sampling_topp -1.0\n",
      "constraints None\n",
      "temperature 1.0\n",
      "diverse_beam_groups -1\n",
      "diverse_beam_strength 0.5\n",
      "diversity_rate -1.0\n",
      "print_alignment None\n",
      "print_step False\n",
      "lm_path None\n",
      "lm_weight 0.0\n",
      "iter_decode_eos_penalty 0.0\n",
      "iter_decode_max_iter 10\n",
      "iter_decode_force_max_iter False\n",
      "iter_decode_with_beam 1\n",
      "iter_decode_with_external_reranker False\n",
      "retain_iter_history False\n",
      "retain_dropout False\n",
      "retain_dropout_modules None\n",
      "decoding_format None\n",
      "no_seed_provided False\n",
      "eos_token None\n",
      "-------------------------------\n",
      "eval_lmarguments:\n",
      "_name None\n",
      "output_word_probs False\n",
      "output_word_stats False\n",
      "context_window 0\n",
      "softmax_batch 9223372036854775807\n",
      "-------------------------------\n",
      "interactivearguments:\n",
      "_name None\n",
      "buffer_size 0\n",
      "input -\n",
      "-------------------------------\n",
      "modelarguments:\n",
      "_name transformer_lm_wiki103\n",
      "activation_fn relu\n",
      "dropout 0.3\n",
      "attention_dropout 0.1\n",
      "activation_dropout 0.1\n",
      "relu_dropout 0.0\n",
      "decoder_embed_dim 1024\n",
      "decoder_output_dim 1024\n",
      "decoder_input_dim 1024\n",
      "decoder_ffn_embed_dim 4096\n",
      "decoder_layers 16\n",
      "decoder_attention_heads 8\n",
      "decoder_normalize_before True\n",
      "no_decoder_final_norm True\n",
      "adaptive_softmax_cutoff 20000,60000\n",
      "adaptive_softmax_dropout 0.2\n",
      "adaptive_softmax_factor 4.0\n",
      "no_token_positional_embeddings False\n",
      "share_decoder_input_output_embed False\n",
      "character_embeddings False\n",
      "character_filters [(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]\n",
      "character_embedding_dim 4\n",
      "char_embedder_highway_layers 2\n",
      "adaptive_input True\n",
      "adaptive_input_factor 4.0\n",
      "adaptive_input_cutoff 20000,60000\n",
      "tie_adaptive_weights True\n",
      "tie_adaptive_proj True\n",
      "decoder_learned_pos False\n",
      "layernorm_embedding False\n",
      "no_scale_embedding False\n",
      "checkpoint_activations False\n",
      "offload_activations False\n",
      "decoder_layerdrop 0.0\n",
      "decoder_layers_to_keep None\n",
      "quant_noise_pq 0.0\n",
      "quant_noise_pq_block_size 8\n",
      "quant_noise_scalar 0.0\n",
      "min_params_to_wrap 100000000\n",
      "base_layers 0\n",
      "base_sublayers 1\n",
      "base_shuffle 1\n",
      "scale_fc False\n",
      "scale_attn False\n",
      "scale_heads False\n",
      "scale_resids False\n",
      "decoder_xformers_att_config None\n",
      "add_bos_token False\n",
      "tokens_per_sample 3072\n",
      "max_target_positions 3072\n",
      "tpu False\n",
      "-------------------------------\n",
      "taskarguments:\n",
      "_name language_modeling\n",
      "data /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2\n",
      "sample_break_mode none\n",
      "tokens_per_sample 3072\n",
      "output_dictionary_size -1\n",
      "self_target False\n",
      "future_target False\n",
      "past_target False\n",
      "add_bos_token False\n",
      "max_target_positions 3072\n",
      "shorten_method none\n",
      "shorten_data_split_list \n",
      "pad_to_fixed_length False\n",
      "pad_to_fixed_bsz False\n",
      "seed 2\n",
      "batch_size None\n",
      "batch_size_valid None\n",
      "dataset_impl None\n",
      "data_buffer_size 0\n",
      "tpu False\n",
      "use_plasma_view True\n",
      "plasma_path /tmp/plasma\n",
      "-------------------------------\n",
      "criterionarguments:\n",
      "_name adaptive_loss\n",
      "sentence_avg False\n",
      "ddp_backend no_c10d\n",
      "-------------------------------\n",
      "optimizerarguments:\n",
      "_name nag\n",
      "momentum 0.99\n",
      "weight_decay 0.0\n",
      "lr [0.0001]\n",
      "-------------------------------\n",
      "lr_schedulerarguments:\n",
      "_name cosine\n",
      "warmup_updates 16000\n",
      "warmup_init_lr 1e-07\n",
      "lr [0.0001]\n",
      "min_lr 0.0\n",
      "t_mult 2.0\n",
      "lr_period_updates 270000.0\n",
      "lr_shrink 0.75\n",
      "max_update 286000\n",
      "-------------------------------\n",
      "scoringarguments:\n",
      "bpearguments:\n",
      "tokenizerarguments:\n",
      "emaarguments:\n",
      "_name None\n",
      "store_ema False\n",
      "ema_decay 0.9999\n",
      "ema_start_update 0\n",
      "ema_seed_model None\n",
      "ema_update_freq 1\n",
      "ema_fp32 False\n",
      "-------------------------------\n",
      "simul_typearguments:\n"
     ]
    }
   ],
   "source": [
    "config = fairseq_model.cfg\n",
    "for k in config.keys():\n",
    "    print(k + 'arguments:')\n",
    "    if config[k] is None:\n",
    "        continue\n",
    "    for l,v in config[k].items():\n",
    "        print(l,v)\n",
    "    print('-------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m270000\u001b[39m, (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m16\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x)\n\u001b[1;32m      6\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnew_full((x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x[:,\u001b[38;5;241m1\u001b[39m:], pad_token),\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "x = torch.randint(0, 270000, (4, 16)).to(device)\n",
    "y = model(x)\n",
    "\n",
    "pad_token = x.new_full((x.size(0),1), 1)\n",
    "target = torch.cat((x[:,1:], pad_token),1)\n",
    "\n",
    "out = model.decoder.adaptive_softmax.get_log_prob(y[0], target=target)\n",
    "idx0 = 0\n",
    "idx1 = 9\n",
    "print(out[idx0,idx1][target[idx0,idx1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 267744])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 18:19:39 | INFO | fairseq.file_utils | loading archive file /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/\n",
      "/home/jovyan/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "2024-08-09 18:19:43 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2024-08-09 18:19:47 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/myleott/tensorboard_logs/2020-05-23/adaptive_lm_wiki103.fp16.no_c10d.transformer_lm_wiki103.adaptive_loss.nag.clip0.1.warmup16000.initlr1e-07.sampletok3072.breaknone.maxtok3072.updatefreq3.seed2.ngpu8', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16480', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3072, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 0, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3072, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 286000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [3], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/myleott/2020-05-23/adaptive_lm_wiki103.fp16.no_c10d.transformer_lm_wiki103.adaptive_loss.nag.clip0.1.warmup16000.initlr1e-07.sampletok3072.breaknone.maxtok3072.updatefreq3.seed2.ngpu8', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_wiki103', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 16, 'decoder_attention_heads': 8, 'decoder_normalize_before': True, 'no_decoder_final_norm': True, 'adaptive_softmax_cutoff': '20000,60000', 'adaptive_softmax_dropout': 0.2, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': False, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': True, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': '20000,60000', 'tie_adaptive_weights': True, 'tie_adaptive_proj': True, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 3072, 'max_target_positions': 3072, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': '/base-vol-2/fairseq/models/adaptive_lm_wiki103.v2', 'sample_break_mode': 'none', 'tokens_per_sample': 3072, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 3072, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 2, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 0, 'tpu': False, 'use_plasma_view': True, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'adaptive_loss', 'sentence_avg': False, 'ddp_backend': 'no_c10d'}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 16000, 'warmup_init_lr': 1e-07, 'lr': [0.0001], 'min_lr': 0.0, 't_mult': 2.0, 'lr_period_updates': 270000.0, 'lr_shrink': 0.75, 'max_update': 286000}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please build Cython components with: `pip install --editable .` or `python setup.py build_ext --inplace`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/fairseq/fairseq/data/token_block_dataset.py:90\u001b[0m, in \u001b[0;36mTokenBlockDataset._build_slice_indices\u001b[0;34m(sizes, break_mode, document_sep_len, block_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_block_utils_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m         _get_slice_indices_fast,\n\u001b[1;32m     92\u001b[0m         _get_block_to_dataset_index_fast,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fairseq.data.token_block_utils_fast'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m fairseq_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mfairseq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBarack Obama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fairseq/fairseq/hub_utils.py:142\u001b[0m, in \u001b[0;36mGeneratorHubInterface.sample\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m], beam: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentences, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    143\u001b[0m     tokenized_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m    144\u001b[0m     batched_hypos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(tokenized_sentences, beam, verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/fairseq/fairseq/hub_utils.py:144\u001b[0m, in \u001b[0;36mGeneratorHubInterface.sample\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample([sentences], beam\u001b[38;5;241m=\u001b[39mbeam, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    143\u001b[0m tokenized_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m--> 144\u001b[0m batched_hypos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(hypos[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m hypos \u001b[38;5;129;01min\u001b[39;00m batched_hypos]\n",
      "File \u001b[0;32m~/fairseq/fairseq/hub_utils.py:199\u001b[0m, in \u001b[0;36mGeneratorHubInterface.generate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, prefix_allowed_tokens_fn, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m inference_step_args \u001b[38;5;241m=\u001b[39m inference_step_args \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    198\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_invalid_size_inputs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    200\u001b[0m     batch \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mapply_to_sample(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch)\n\u001b[1;32m    201\u001b[0m     translations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39minference_step(\n\u001b[1;32m    202\u001b[0m         generator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minference_step_args\n\u001b[1;32m    203\u001b[0m     )\n",
      "File \u001b[0;32m~/fairseq/fairseq/hub_utils.py:287\u001b[0m, in \u001b[0;36mGeneratorHubInterface._build_batches\u001b[0;34m(self, tokens, skip_invalid_size_inputs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_batches\u001b[39m(\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m, tokens: List[List[\u001b[38;5;28mint\u001b[39m]], skip_invalid_size_inputs: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    285\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens])\n\u001b[1;32m    286\u001b[0m     batch_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mget_batch_iterator(\n\u001b[0;32m--> 287\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_dataset_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    288\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    289\u001b[0m         max_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    290\u001b[0m         max_positions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_positions,\n\u001b[1;32m    291\u001b[0m         ignore_invalid_inputs\u001b[38;5;241m=\u001b[39mskip_invalid_size_inputs,\n\u001b[1;32m    292\u001b[0m         disable_iterator_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    293\u001b[0m     )\u001b[38;5;241m.\u001b[39mnext_epoch_itr(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_iterator\n",
      "File \u001b[0;32m~/fairseq/fairseq/tasks/language_modeling.py:276\u001b[0m, in \u001b[0;36mLanguageModelingTask.build_dataset_for_inference\u001b[0;34m(self, src_tokens, src_lengths, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset_for_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_tokens, src_lengths, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Generate batches for inference. We prepend an eos token to src_tokens\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    (or bos if `--add-bos-token` is set) and we append a <pad> to target.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    This is convenient both for generation with a prefix and LM scoring.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m StripTokenDataset(\n\u001b[0;32m--> 276\u001b[0m         \u001b[43mTokenBlockDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ignored for \"eos\" break mode\u001b[39;49;00m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43meos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbreak_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;66;03m# remove eos from (end of) target sequence\u001b[39;00m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dictionary\u001b[38;5;241m.\u001b[39meos(),\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m     src_dataset \u001b[38;5;241m=\u001b[39m PrependTokenDataset(\n\u001b[1;32m    288\u001b[0m         dataset,\n\u001b[1;32m    289\u001b[0m         token\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m         ),\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m     tgt_dataset \u001b[38;5;241m=\u001b[39m AppendTokenDataset(dataset, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dictionary\u001b[38;5;241m.\u001b[39mpad())\n",
      "File \u001b[0;32m~/fairseq/fairseq/data/token_block_dataset.py:60\u001b[0m, in \u001b[0;36mTokenBlockDataset.__init__\u001b[0;34m(self, dataset, sizes, block_size, pad, eos, break_mode, include_targets, document_sep_len, use_plasma_view, split_path, plasma_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(sizes)\n\u001b[0;32m---> 60\u001b[0m _sizes, block_to_dataset_index, slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_slice_indices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreak_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_sep_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_plasma_view:\n\u001b[1;32m     64\u001b[0m     plasma_id \u001b[38;5;241m=\u001b[39m (block_size, document_sep_len, \u001b[38;5;28mstr\u001b[39m(break_mode), \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "File \u001b[0;32m~/fairseq/fairseq/data/token_block_dataset.py:95\u001b[0m, in \u001b[0;36mTokenBlockDataset._build_slice_indices\u001b[0;34m(sizes, break_mode, document_sep_len, block_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_block_utils_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m         _get_slice_indices_fast,\n\u001b[1;32m     92\u001b[0m         _get_block_to_dataset_index_fast,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build Cython components with: `pip install --editable .` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `python setup.py build_ext --inplace`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sizes, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    101\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sizes, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "\u001b[0;31mImportError\u001b[0m: Please build Cython components with: `pip install --editable .` or `python setup.py build_ext --inplace`"
     ]
    }
   ],
   "source": [
    "from fairseq.models.transformer_lm import TransformerLanguageModel\n",
    "fairseq_model = TransformerLanguageModel.from_pretrained('/base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/', 'model.pt')\n",
    "device = \"cuda:0\"\n",
    "\n",
    "fairseq_model.to(device)\n",
    "fairseq_model.sample('Barack Obama', beam=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1296.13s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikitext-103.tar.gz already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "#First download and prepare the WikiText-103 dataset:\n",
    "!(cd examples/language_model; \\\n",
    "bash prepare-wikitext-103.sh; \\\n",
    "cd ../..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1330.11s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 22:51:40 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-08-05 22:51:41 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wikitext-103', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict=None, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='examples/language_model/wikitext-103/wiki.test.tokens', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='examples/language_model/wikitext-103/wiki.train.tokens', use_plasma_view=False, user_dir=None, validpref='examples/language_model/wikitext-103/wiki.valid.tokens', wandb_project=None, workers=20)\n",
      "2024-08-05 22:52:34 | INFO | fairseq_cli.preprocess | [None] Dictionary: 267744 types\n",
      "2024-08-05 22:56:19 | INFO | fairseq_cli.preprocess | [None] examples/language_model/wikitext-103/wiki.train.tokens: 1801350 sents, 103227021 tokens, 0.0% replaced (by <unk>)\n",
      "2024-08-05 22:56:19 | INFO | fairseq_cli.preprocess | [None] Dictionary: 267744 types\n",
      "2024-08-05 22:56:21 | INFO | fairseq_cli.preprocess | [None] examples/language_model/wikitext-103/wiki.valid.tokens: 3760 sents, 217646 tokens, 0.0% replaced (by <unk>)\n",
      "2024-08-05 22:56:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 267744 types\n",
      "2024-08-05 22:56:24 | INFO | fairseq_cli.preprocess | [None] examples/language_model/wikitext-103/wiki.test.tokens: 4358 sents, 245569 tokens, 0.0% replaced (by <unk>)\n",
      "2024-08-05 22:56:24 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wikitext-103\n"
     ]
    }
   ],
   "source": [
    "#Next preprocess/binarize the data:\n",
    "!(TEXT=examples/language_model/wikitext-103; \\\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --trainpref $TEXT/wiki.train.tokens \\\n",
    "    --validpref $TEXT/wiki.valid.tokens \\\n",
    "    --testpref $TEXT/wiki.test.tokens \\\n",
    "    --destdir data-bin/wikitext-103 \\\n",
    "    --workers 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.44s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 22:29:35 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-08-05 22:29:37 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/model.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 400, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': 2, 'batch_size_valid': 2, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/fairseq/bin/fairseq-eval-lm\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/jovyan/fairseq/fairseq_cli/eval_lm.py\", line 343, in cli_main\n",
      "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
      "  File \"/home/jovyan/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/jovyan/fairseq/fairseq_cli/eval_lm.py\", line 249, in main\n",
      "    task = tasks.setup_task(cfg.task)\n",
      "  File \"/home/jovyan/fairseq/fairseq/tasks/__init__.py\", line 47, in setup_task\n",
      "    return task.setup_task(cfg, **kwargs)\n",
      "  File \"/home/jovyan/fairseq/fairseq/tasks/language_modeling.py\", line 171, in setup_task\n",
      "    dictionary, output_dictionary = cls.setup_dictionary(args, **kwargs)\n",
      "  File \"/home/jovyan/fairseq/fairseq/tasks/language_modeling.py\", line 155, in setup_dictionary\n",
      "    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\n",
      "  File \"/home/jovyan/fairseq/fairseq/data/dictionary.py\", line 228, in load\n",
      "    d.add_from_file(f)\n",
      "  File \"/home/jovyan/fairseq/fairseq/data/dictionary.py\", line 241, in add_from_file\n",
      "    raise fnfe\n",
      "  File \"/home/jovyan/fairseq/fairseq/data/dictionary.py\", line 238, in add_from_file\n",
      "    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/wikitext-103/dict.txt'\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "# Evaluate\n",
    "\n",
    "# Add your code here\n",
    "!(fairseq-eval-lm data-bin/wikitext-103 \\\n",
    "    --path /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/model.pt \\\n",
    "    --batch-size 2 \\\n",
    "    --tokens-per-sample 512 \\\n",
    "    --context-window 400)\n",
    "# | Evaluated 245569 tokens in 56.1s (4379.02 tokens/s)\n",
    "# | Loss: 3.4164, Perplexity: 30.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cli_main() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq_cli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_lm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cli_main\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcli_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfairseq-eval-lm data-bin/wikitext-103 \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m    --path /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/model.pt \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m    --batch-size 2 \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    --tokens-per-sample 512 \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m    --context-window 400\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cli_main() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from fairseq_cli.eval_lm import cli_main\n",
    "\n",
    "cli_main(\"fairseq-eval-lm data-bin/wikitext-103 \\\n",
    "    --path /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/model.pt \\\n",
    "    --batch-size 2 \\\n",
    "    --tokens-per-sample 512 \\\n",
    "    --context-window 400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 22:42:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-08-05 22:42:19 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/model.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 400, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': 2, 'batch_size_valid': 2, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/fairseq/bin/fairseq-eval-lm\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/jovyan/fairseq/fairseq_cli/eval_lm.py\", line 343, in cli_main\n",
      "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
      "  File \"/home/jovyan/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/home/jovyan/fairseq/fairseq_cli/eval_lm.py\", line 249, in main\n",
      "    task = tasks.setup_task(cfg.task)\n",
      "  File \"/home/jovyan/fairseq/fairseq/tasks/__init__.py\", line 47, in setup_task\n",
      "    return task.setup_task(cfg, **kwargs)\n",
      "  File \"/home/jovyan/fairseq/fairseq/tasks/language_modeling.py\", line 171, in setup_task\n",
      "    dictionary, output_dictionary = cls.setup_dictionary(args, **kwargs)\n",
      "  File \"/home/jovyan/fairseq/fairseq/tasks/language_modeling.py\", line 155, in setup_dictionary\n",
      "    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\n",
      "  File \"/home/jovyan/fairseq/fairseq/data/dictionary.py\", line 228, in load\n",
      "    d.add_from_file(f)\n",
      "  File \"/home/jovyan/fairseq/fairseq/data/dictionary.py\", line 241, in add_from_file\n",
      "    raise fnfe\n",
      "  File \"/home/jovyan/fairseq/fairseq/data/dictionary.py\", line 238, in add_from_file\n",
      "    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/wikitext-103/dict.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define your command as a string\n",
    "# Replace 'arg1', 'arg2', etc. with your actual arguments\n",
    "command = \"fairseq-eval-lm data-bin/wikitext-103 \\\n",
    "    --path /base-vol-2/fairseq/models/adaptive_lm_wiki103.v2/model.pt \\\n",
    "    --batch-size 2 \\\n",
    "    --tokens-per-sample 512 \\\n",
    "    --context-window 400\"\n",
    "\n",
    "# Use subprocess to run the command\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n",
    "process.wait()\n",
    "\n",
    "# Print the output\n",
    "print(process.stdout.read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
